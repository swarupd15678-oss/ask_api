# -*- coding: utf-8 -*-
"""Groqclone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BeyN04he8x5KEYVHdxTVhlai2e0WAGC7
"""

# Install necessary libraries
!pip install -q langchain langchain-groq langchain-chroma langchain-community beautifulsoup4 sentence-transformers
print("Libraries installed successfully!")

import os
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser




GROQ_API_KEY = "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"

# 1. Install necessary libraries
!pip install -q langchain langchain-groq langchain-chroma langchain-community beautifulsoup4 sentence-transformers

# Define API Key variable for later use
GROQ_API_KEY = "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"
print(" Libraries installed and API Key set.")

from langchain_community.document_loaders import WebBaseLoader

print("- Loading website content -")
loader = WebBaseLoader(web_paths=["https://celoref.mintlify.app/overview/use-cases"])
docs = loader.load()

# PRINTING OUTPUT
print(f"Successfully loaded {len(docs)} document(s).")
print(f"Content Preview (First 500 chars):\n{docs[0].page_content[:500]}...")

# Import ONLY the splitter
from langchain_text_splitters import RecursiveCharacterTextSplitter

print("Splitting text into chunks ")

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,   # Size of each piece
    chunk_overlap=20  # How much previous text to repeat
)
splits = text_splitter.split_documents(docs)

# PRINTING OUTPUT
print(f"Original Documents: {len(docs)}")
print(f"Total Split Chunks: {len(splits)}")

print("\n  Chunk #1  ")
print(splits[0].page_content)

print("\n Chunk #2 (Check the start  it should repeat the end of Chunk #1) ")
print(splits[1].page_content)

from langchain_community.embeddings import HuggingFaceEmbeddings

print("--- Initializing Embedding Model ---")
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
print("✅ Model loaded: sentence-transformers/all-MiniLM-L6-v2")

# Import ONLY vector store
from langchain_chroma import Chroma



# This creates the database in-memory (RAM) automatically
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings
)

print("✅ Vector Store created successfully.")

# PRINTING COLLECTION DETAILS
print("\n--- Inspecting Chroma Collection ")
# .get() allows us to look inside the database
collection_data = vectorstore.get()

print(f"Total items in collection: {len(collection_data['ids'])}")
print(f"Sample ID: {collection_data['ids'][0]}")
# Note: The actual vectors are hidden for efficiency, but the data is there.

# Import ONLY LLM
from langchain_groq import ChatGroq

print("--- Initializing LLM ---")
llm = ChatGroq(
    model="openai/gpt-oss-20b", # Standard Groq Model
    temperature=0,
    api_key=GROQ_API_KEY
)
print("✅ LLM Ready.")

# Import ONLY Chain components
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# 1. Define Prompt
template = """Answer ONLY from this context:

{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

# 2. Setup Retriever
retriever = vectorstore.as_retriever()

# 3. Helper function to join docs
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# 4. Build Chain
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# 5. Run
query = "how to implement celoref sdk"
print(f"Query: {query}\n")
print("-" * 30)

response = rag_chain.invoke(query)
print(response)
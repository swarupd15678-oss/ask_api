# Install necessary libraries
!pip install -q langchain langchain-groq langchain-chroma langchain-community beautifulsoup4 sentence-transformers
print("Libraries installed successfully!")
import os
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser




GROQ_API_KEY = "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"

# 1. Install necessary libraries
!pip install -q langchain langchain-groq langchain-chroma langchain-community beautifulsoup4 sentence-transformers

# Define API Key variable for later use
GROQ_API_KEY = "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"
print(" Libraries installed and API Key set.")

from langchain_community.document_loaders import WebBaseLoader

print("- Loading website content -")
loader = WebBaseLoader(web_paths=["https://celoref.mintlify.app/overview/use-cases"])
docs = loader.load()

# PRINTING OUTPUT
print(f"Successfully loaded {len(docs)} document(s).")
print(f"Content Preview (First 500 chars):\n{docs[0].page_content[:500]}...")
# Import ONLY the splitter
from langchain_text_splitters import RecursiveCharacterTextSplitter

print("Splitting text into chunks ")

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,   # Size of each piece
    chunk_overlap=20  # How much previous text to repeat
)
splits = text_splitter.split_documents(docs)

# PRINTING OUTPUT
print(f"Original Documents: {len(docs)}")
print(f"Total Split Chunks: {len(splits)}")

print("\n  Chunk #1  ")
print(splits[0].page_content)

print("\n Chunk #2 (Check the start  it should repeat the end of Chunk #1) ")
print(splits[1].page_content)
# Import ONLY the splitter
from langchain_text_splitters import RecursiveCharacterTextSplitter

print("Splitting text into chunks ")

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,   # Size of each piece
    chunk_overlap=20  # How much previous text to repeat
)
splits = text_splitter.split_documents(docs)

# PRINTING OUTPUT
print(f"Original Documents: {len(docs)}")
print(f"Total Split Chunks: {len(splits)}")

print("\n  Chunk #1  ")
print(splits[0].page_content)

print("\n Chunk #2 (Check the start  it should repeat the end of Chunk #1) ")
print(splits[1].page_content)

from langchain_community.embeddings import HuggingFaceEmbeddings

print("--- Initializing Embedding Model ---")
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
print("✅ Model loaded: sentence-transformers/all-MiniLM-L6-v2")
--- Initializing Embedding Model ---
/tmp/ipython-input-1345594438.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.
  embeddings = HuggingFaceEmbeddings(
/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: 
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
modules.json: 100%
 349/349 [00:00<00:00, 24.6kB/s]
config_sentence_transformers.json: 100%
 116/116 [00:00<00:00, 4.72kB/s]
README.md: 
 10.5k/? [00:00<00:00, 344kB/s]
sentence_bert_config.json: 100%
 53.0/53.0 [00:00<00:00, 5.00kB/s]
config.json: 100%
 612/612 [00:00<00:00, 34.7kB/s]
model.safetensors: 100%
 90.9M/90.9M [00:01<00:00, 82.7MB/s]
tokenizer_config.json: 100%
 350/350 [00:00<00:00, 6.89kB/s]
vocab.txt: 
 232k/? [00:00<00:00, 3.25MB/s]
tokenizer.json: 
 466k/? [00:00<00:00, 9.87MB/s]
special_tokens_map.json: 100%
 112/112 [00:00<00:00, 2.25kB/s]
config.json: 100%
 190/190 [00:00<00:00, 5.72kB/s]
✅ Model loaded: sentence-transformers/all-MiniLM-L6-v2
# Import ONLY vector store
from langchain_chroma import Chroma



# This creates the database in-memory (RAM) automatically
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings
)

print("✅ Vector Store created successfully.")

# PRINTING COLLECTION DETAILS
print("\n--- Inspecting Chroma Collection ")
# .get() allows us to look inside the database
collection_data = vectorstore.get()

print(f"Total items in collection: {len(collection_data['ids'])}")
print(f"Sample ID: {collection_data['ids'][0]}")
# Note: The actual vectors are hidden for efficiency, but the data is there.
# Import ONLY vector store
from langchain_chroma import Chroma



# This creates the database in-memory (RAM) automatically
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings
)

print("✅ Vector Store created successfully.")

# PRINTING COLLECTION DETAILS
print("\n--- Inspecting Chroma Collection ")
# .get() allows us to look inside the database
collection_data = vectorstore.get()

print(f"Total items in collection: {len(collection_data['ids'])}")
print(f"Sample ID: {collection_data['ids'][0]}")
# Note: The actual vectors are hidden for efficiency, but the data is there.
# Import ONLY LLM
from langchain_groq import ChatGroq

print("--- Initializing LLM ---")
llm = ChatGroq(
    model="openai/gpt-oss-20b", # Standard Groq Model
    temperature=0,
    api_key=GROQ_API_KEY
)
print("✅ LLM Ready.")
# Import ONLY Chain components
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# 1. Define Prompt
template = """Answer ONLY from this context:

{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

# 2. Setup Retriever
retriever = vectorstore.as_retriever()

# 3. Helper function to join docs
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# 4. Build Chain
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# 5. Run
query = "how to implement celoref sdk"
print(f"Query: {query}\n")
print("-" * 30)

response = rag_chain.invoke(query)
print(response)
